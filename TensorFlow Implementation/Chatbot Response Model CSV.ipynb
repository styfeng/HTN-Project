{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "# things we need for NLP\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# things we need for Tensorflow\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: ['a', 'about', 'ach', 'aft', 'ago', 'al', 'allergenswhy', 'almost', 'alon', 'along', 'alot', 'already', 'also', 'although', 'alway', 'am', 'an', 'and', 'angry', 'ankl', 'annoy', 'antibiot', 'any', 'anyth', 'ap', 'appear', 'appetit', 'appl', 'ar', 'area', 'arm', 'around', 'as', 'at', 'away', 'back', 'bad', 'bank', 'bar', 'be', 'becam', 'becaus', 'becom', 'been', 'beg', 'begin', 'believ', 'bend', 'between', 'big', 'bit', 'blee', 'blist', 'block', 'blood', 'body', 'brea', 'breast', 'bright', 'bump', 'bunch', 'burn', 'but', 'by', 'cafeter', 'cal', 'can', 'catch', 'caught', 'chest', 'chlamydia', 'cle', 'coat', 'cold', 'com', 'coma', 'commun', 'complet', 'concern', 'condit', 'confus', 'const', 'contact', 'cough', 'could', 'couldnt', 'cramp', 'cri', 'cry', 'cystit', 'day', 'daysit', 'develop', 'did', 'didnt', 'difficul', 'difficult', 'dimin', 'din', 'dirty', 'discharg', 'discov', 'diseas', 'dizzy', 'do', 'doct', 'doe', 'doing', 'dont', 'down', 'dread', 'dry', 'dur', 'eat', 'end', 'enemy', 'entir', 'ev', 'every', 'everyth', 'exceiv', 'expery', 'expiery', 'extr', 'extrem', 'ey', 'fac', 'facesh', 'faimili', 'fal', 'fat', 'feath', 'feel', 'felt', 'fev', 'few', 'fish', 'flow', 'food', 'for', 'forehead', 'found', 'frequ', 'friend', 'from', 'ful', 'funny', 'furtherm', 'gabriel', 'gam', 'gav', 'genit', 'get', 'girlfriend', 'go', 'going', 'got', 'gre', 'grind', 'guess', 'gym', 'hack', 'had', 'halfway', 'hamst', 'hand', 'hard', 'hardalmost', 'has', 'hav', 'he', 'head', 'headach', 'heartburn', 'hell', 'hello', 'help', 'her', 'hey', 'hi', 'high', 'him', 'his', 'hom', 'hous', 'how', 'hurt', 'i', 'ic', 'if', 'il', 'im', 'import', 'in', 'indigest', 'infect', 'init', 'insid', 'instead', 'into', 'irrit', 'is', 'it', 'itch', 'itchy', 'iv', 'joint', 'jungl', 'just', 'kept', 'know', 'last', 'lat', 'leav', 'lif', 'light', 'lik', 'lip', 'liquid', 'littl', 'lookdiff', 'loss', 'lot', 'loud', 'lumpy', 'many', 'mat', 'may', 'me', 'meal', 'meas', 'might', 'milky', 'min', 'mir', 'monthwh', 'mor', 'morn', 'mosquito', 'most', 'moth', 'mou', 'mov', 'movy', 'much', 'musc', 'my', 'myself', 'naus', 'nee', 'nev', 'next', 'night', 'nippl', 'nipplesshould', 'no', 'nor', 'nos', 'not', 'numb', 'occas', 'occass', 'occur', 'of', 'off', 'oft', 'on', 'ont', 'or', 'oth', 'out', 'ov', 'pain', 'park', 'part', 'past', 'pay', 'pee', 'peopl', 'period', 'pick', 'pink', 'piss', 'plac', 'play', 'pool', 'poop', 'prep', 'pretty', 'pric', 'project', 'prop', 'puberty', 'puk', 'purg', 'quit', 'random', 'rash', 'raz', 'real', 'rec', 'red', 'refus', 'rememb', 'rend', 'rep', 'requir', 'resta', 'retriev', 'room', 'runny', 'sad', 'sandwich', 'saw', 'say', 'school', 'seem', 'sens', 'sensationâ€i', 'sery', 'sev', 'sex', 'should', 'show', 'shredding', 'sid', 'singl', 'sint', 'skin', 'smal', 'sneez', 'sniff', 'so', 'socc', 'som', 'someon', 'someth', 'sometim', 'sor', 'sour', 'spat', 'speak', 'spot', 'standup', 'start', 'stay', 'stomach', 'straight', 'strange', 'street', 'sunk', 'suppos', 'sur', 'surrend', 'surround', 'swel', 'swim', 'swol', 'symptom', 'tak', 'talk', 'task', 'tast', 'temp', 'that', 'the', 'them', 'then', 'ther', 'thes', 'they', 'thi', 'thing', 'think', 'thos', 'though', 'thought', 'three', 'throat', 'through', 'througout', 'tight', 'tim', 'tir', 'to', 'today', 'toilet', 'too', 'touch', 'transmit', 'trip', 'troubl', 'turn', 'uncomfort', 'uncontrol', 'up', 'urin', 'useless', 'vagin', 'very', 'vessel', 'vis', 'visit', 'vomit', 'walk', 'was', 'washroom', 'we', 'weak', 'wear', 'week', 'weekend', 'weird', 'wel', 'went', 'wer', 'what', 'when', 'wher', 'which', 'whil', 'whit', 'why', 'with', 'wok', 'wont', 'worry', 'would', 'wrong', 'yellowgreen', 'yesterday', 'you', 'zoo']\n",
      "classes: ['Allergies', 'Alzheimer', 'Breast Cancer', 'Chickenpox', 'Chlamydia', 'Common Cold', 'Conjunctivitis', 'Coronary Heart Disease', 'E Coli Infection', 'Eating Disorders including Anorexia & Bulimia Nervosa', 'Hepatitis B', 'Pregnancy', 'Prostate Cancer', 'West Nile Virus']\n",
      "classes details: [{'danger level': '1', 'treatments': 'Stay hydrated;rest;sooth a sore throat using saltwater gargle;take over the counter cold and cough medications\\n', 'class': 'Common Cold', 'synonyms': 'Upper Respiratory Tract;Nose and Throat Infection'}, {'danger level': '1', 'treatments': 'Remove the cause of allergy; \\notherwise take Antihistamines to relieve sneezing;Decongestants to relieve congestions in nasal membranes;Anti inflamary agents to reduce;Allergy shots', 'class': 'Allergies', 'synonyms': 'Allergic Reaction'}, {'danger level': '1', 'treatments': 'See a doctor;Bacterial cases can be treated with antibiotic eye drops;Allergic reactions can be treated with other eye dro', 'class': 'Conjunctivitis', 'synonyms': 'Pink Eye'}, {'danger level': '1', 'treatments': 'Visit a hospital immediately', 'class': 'E Coli Infection', 'synonyms': ''}, {'danger level': '4', 'treatments': 'See a professional doctor;Use Azithromycin or Doxycycline', 'class': 'Chlamydia', 'synonyms': 'Sexually Transmitted Infection (STI)'}, {'danger level': '6', 'treatments': 'See a doctor immediately;Take antiviral drugs', 'class': 'Hepatitis B', 'synonyms': 'Herpes'}, {'danger level': '8', 'treatments': 'Please see a healthcare professional;There are no specific treatments for West Nile Virus in humans', 'class': 'West Nile Virus', 'synonyms': ''}, {'danger level': '2', 'treatments': 'Drink plenty of fluid;Take Tylenol;Consult with a doctor', 'class': 'Chickenpox', 'synonyms': 'Varicella Zoster Virus'}, {'danger level': '4', 'treatments': 'Cholinesterase inhibitors;Memantine(Namenda);Please consult with a professional doctor', 'class': 'Alzheimer', 'synonyms': ''}, {'danger level': '2', 'treatments': 'You are pregnant! Remember to take your prenatal vitamin;quit smoking;stop drinking alcohol;cut down on caffeine; avoid hazardous foods;eat well and sleep well!', 'class': 'Pregnancy', 'synonyms': ''}, {'danger level': '3', 'treatments': 'Seek help from a therapist, physician, and nutritionist', 'class': 'Eating Disorders including Anorexia & Bulimia Nervosa', 'synonyms': ''}, {'danger level': '7', 'treatments': 'quit smoking;have a healthy diet;exercise regularly;consult with a doctor', 'class': 'Coronary Heart Disease', 'synonyms': ''}, {'danger level': '10', 'treatments': 'You may have breast cancer, please consult with a doctor and conduct further diagnostics at your local hospital', 'class': 'Breast Cancer', 'synonyms': ''}, {'danger level': '10', 'treatments': 'You may have prostate cancer, please consult with a doctor and conduct further diagnostics at your local hospital', 'class': 'Prostate Cancer', 'synonyms': ''}]\n"
     ]
    }
   ],
   "source": [
    "# restore all of our data structures\n",
    "import pickle\n",
    "data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
    "words = data['words']\n",
    "print(\"words:\",words)\n",
    "classes = data['classes']\n",
    "print(\"classes:\",classes)\n",
    "train_x = data['train_x']\n",
    "train_y = data['train_y']\n",
    "classes_details = data['classes_details']\n",
    "print(\"classes details:\",classes_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.response\n",
    "import sys\n",
    "import os, glob\n",
    "import http.client, urllib\n",
    "import json\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64\n",
    "\n",
    "# Replace the accessKey string value with your valid access key.\n",
    "accessKey = '82e4612615634c398bfd26e7d6327833'\n",
    "url = 'westcentralus.api.cognitive.microsoft.com'\n",
    "path = '/text/analytics/v2.0/keyPhrases'\n",
    "\n",
    "def extract_keywords(body):\n",
    "    headers = {'Ocp-Apim-Subscription-Key': accessKey}\n",
    "    conn = http.client.HTTPSConnection(url)\n",
    "    body_req = {'documents':[{'language': 'en', 'id': 1, 'text': body}]}\n",
    "    body_json = json.dumps(body_req)\n",
    "    conn.request (\"POST\", path, body_json, headers)\n",
    "    response = conn.getresponse ()\n",
    "    string = response.read().decode('utf-8')\n",
    "    json_obj = json.loads(string)\n",
    "    print(\"json_obj: \", json_obj)\n",
    "    keyphrases_list = ((json_obj['documents'])[0])['keyPhrases']\n",
    "    print(\"keyphrases_list: \", keyphrases_list)\n",
    "    return keyphrases_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\steven\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tflearn\\objectives.py:66: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# reset underlying graph data\n",
    "tf.reset_default_graph()\n",
    "# Build neural network\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net, 200)\n",
    "net = tflearn.fully_connected(net, 100)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "# Define model and setup tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a table structure to hold the different punctuation used\n",
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                    if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "## method to remove punctuations from sentences.\n",
    "def remove_punctuation(text):\n",
    "     return text.translate(tbl)\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "    #strip punctuations, numbers, and words containing numbers\n",
    "    no_punct = remove_punctuation(sentence)\n",
    "    no_nums1 = ' '.join(s for s in no_punct.split() if not any(c.isdigit() for c in s))\n",
    "    no_nums2 = re.sub(r'\\d+', '', no_nums1)\n",
    "    keywords = extract_keywords(no_nums2)\n",
    "    keyword_str = ' '.join(s for s in keywords)\n",
    "    # tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(keyword_str)\n",
    "    # stem each word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    print(\"sentence after stemming: \", sentence_words)\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json_obj:  {'documents': [{'keyPhrases': ['stuffy nose', 'throat', 'days', 'sneezing', 'severe cough'], 'id': '1'}], 'errors': []}\n",
      "keyphrases_list:  ['stuffy nose', 'throat', 'days', 'sneezing', 'severe cough']\n",
      "sentence after stemming:  ['stuffy', 'nos', 'throat', 'day', 'sneez', 'sev', 'cough']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "['Allergies', 'Alzheimer', 'Breast Cancer', 'Chickenpox', 'Chlamydia', 'Common Cold', 'Conjunctivitis', 'Coronary Heart Disease', 'E Coli Infection', 'Eating Disorders including Anorexia & Bulimia Nervosa', 'Hepatitis B', 'Pregnancy', 'Prostate Cancer', 'West Nile Virus']\n"
     ]
    }
   ],
   "source": [
    "p = bow(\"I have recently had a very runny and stuffy nose. The last few days, my throat has also been very sore and I have also developed a severe cough with lots of sneezing\", words)\n",
    "print (p)\n",
    "print (classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Steven\\Documents\\HTN v. 4\\model.tflearn\n"
     ]
    }
   ],
   "source": [
    "# load our saved model\n",
    "model.load('./model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR_THRESHOLD = 0.01\n",
    "def classify(sentence):\n",
    "    # generate probabilities from the model\n",
    "    results = model.predict([bow(sentence, words)])[0]\n",
    "    #print(results)\n",
    "    # filter out predictions below a threshold\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]], r[1]))\n",
    "    # return tuple of intent and probability\n",
    "    return return_list\n",
    "\n",
    "def more_response(key):\n",
    "    if key in [\"yes\", \"of course\", \"sure\", \"certainly\", \"absolutely\", \"yeah\", \"yep\", \"yup\", \"ya\", \"yea\", \"aye\"]:\n",
    "        sentence_new = input(\"Okay, please tell me any possible symptoms. The more details, the better!\")\n",
    "        return response(sentence_new,0)\n",
    "    elif key in [\"negative\", \"nope\", \"no\", \"nah\", \"no way\", \"nay\"]:\n",
    "        return print(\"Okay, thanks for visiting iDoctor. Enjoy the rest of your day!\")\n",
    "    else:\n",
    "        more = input(\"Sorry, I couldn't understand that. Would you like more diagnosis?\")\n",
    "        return more_response(more.strip().lower())\n",
    "\n",
    "def response(sentence, counter):\n",
    "    if counter <= 2:\n",
    "        results = classify(sentence)\n",
    "    # if we have a classification then find the matching intent tag\n",
    "        #if results and len(sentence) >= 10:\n",
    "            # loop as long as there are matches to process\n",
    "            #print(\"Here are some likely conditions you may have:\\n\")\n",
    "            #for i in range(0,min(3,len(results))):\n",
    "                #print((results[i])[0] + \": \" + classes_details[(results[i])[0]])\n",
    "           # more = input(\"Is there anything else you would like me to diagnose?\")\n",
    "            #return more_response(more.strip().lower())\n",
    "#         else:\n",
    "#             counter += 1\n",
    "#             sentence_add = input(\"Unfortunately, I do not have enough information. Please tell me more!\")\n",
    "#             sentence = sentence + \" \" + sentence_add\n",
    "#             print(\"SENTENCE UP UNTIL THIS POINT: \" + sentence)\n",
    "#             return response(sentence, counter)\n",
    "    else:\n",
    "        total_responses = \"\"\n",
    "        return print(\"Unfortunately, I cannot help diagnose your medical condition. Please consult a medical professional such as your family doctor. Enjoy the rest of your day!\")\n",
    "\n",
    "def main():\n",
    "    print(\"Hi there, my name is iDoctor. Today I will be helping to diagnose any possible medical conditions you may have. Let's get started!\")\n",
    "    name = input(\"Firstly, what is your name?\")\n",
    "    sentence = input(\"Hi \" + name + \"! Please tell me how you are feeling, and any possible symptoms you may be experiencing. The more details, the better!\")\n",
    "    response(sentence, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json_obj:  {'documents': [{'keyPhrases': ['muscle weakness rash'], 'id': '1'}], 'errors': []}\n",
      "keyphrases_list:  ['muscle weakness rash']\n",
      "sentence after stemming:  ['musc', 'weak', 'rash']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('West Nile Virus', 0.08243078),\n",
       " ('Conjunctivitis', 0.07591157),\n",
       " ('Alzheimer', 0.075225554),\n",
       " ('Pregnancy', 0.0746474),\n",
       " ('Coronary Heart Disease', 0.0737862),\n",
       " ('Chickenpox', 0.072993614),\n",
       " ('E Coli Infection', 0.072947465),\n",
       " ('Prostate Cancer', 0.07134312),\n",
       " ('Eating Disorders including Anorexia & Bulimia Nervosa', 0.06943686),\n",
       " ('Common Cold', 0.06803922),\n",
       " ('Allergies', 0.06753826),\n",
       " ('Chlamydia', 0.067349195),\n",
       " ('Hepatitis B', 0.065131105),\n",
       " ('Breast Cancer', 0.06321975)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(\"muscle weakness, rash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'ach', 'aft', 'ago', 'al', 'allergenswhy', 'almost', 'alon', 'along', 'alot', 'already', 'also', 'although', 'alway', 'am', 'an', 'and', 'angry', 'ankl', 'annoy', 'antibiot', 'any', 'anyth', 'ap', 'appear', 'appetit', 'appl', 'ar', 'area', 'arm', 'around', 'as', 'at', 'away', 'back', 'bad', 'bank', 'bar', 'be', 'becam', 'becaus', 'becom', 'been', 'beg', 'begin', 'believ', 'bend', 'between', 'big', 'bit', 'blee', 'blist', 'block', 'blood', 'body', 'brea', 'breast', 'bright', 'bump', 'bunch', 'burn', 'but', 'by', 'cafeter', 'cal', 'can', 'catch', 'caught', 'chest', 'chlamydia', 'cle', 'coat', 'cold', 'com', 'coma', 'commun', 'complet', 'concern', 'condit', 'confus', 'const', 'contact', 'cough', 'could', 'couldnt', 'cramp', 'cri', 'cry', 'cystit', 'day', 'daysit', 'develop', 'did', 'didnt', 'difficul', 'difficult', 'dimin', 'din', 'dirty', 'discharg', 'discov', 'diseas', 'dizzy', 'do', 'doct', 'doe', 'doing', 'dont', 'down', 'dread', 'dry', 'dur', 'eat', 'end', 'enemy', 'entir', 'ev', 'every', 'everyth', 'exceiv', 'expery', 'expiery', 'extr', 'extrem', 'ey', 'fac', 'facesh', 'faimili', 'fal', 'fat', 'feath', 'feel', 'felt', 'fev', 'few', 'fish', 'flow', 'food', 'for', 'forehead', 'found', 'frequ', 'friend', 'from', 'ful', 'funny', 'furtherm', 'gabriel', 'gam', 'gav', 'genit', 'get', 'girlfriend', 'go', 'going', 'got', 'gre', 'grind', 'guess', 'gym', 'hack', 'had', 'halfway', 'hamst', 'hand', 'hard', 'hardalmost', 'has', 'hav', 'he', 'head', 'headach', 'heartburn', 'hell', 'hello', 'help', 'her', 'hey', 'hi', 'high', 'him', 'his', 'hom', 'hous', 'how', 'hurt', 'i', 'ic', 'if', 'il', 'im', 'import', 'in', 'indigest', 'infect', 'init', 'insid', 'instead', 'into', 'irrit', 'is', 'it', 'itch', 'itchy', 'iv', 'joint', 'jungl', 'just', 'kept', 'know', 'last', 'lat', 'leav', 'lif', 'light', 'lik', 'lip', 'liquid', 'littl', 'lookdiff', 'loss', 'lot', 'loud', 'lumpy', 'many', 'mat', 'may', 'me', 'meal', 'meas', 'might', 'milky', 'min', 'mir', 'monthwh', 'mor', 'morn', 'mosquito', 'most', 'moth', 'mou', 'mov', 'movy', 'much', 'musc', 'my', 'myself', 'naus', 'nee', 'nev', 'next', 'night', 'nippl', 'nipplesshould', 'no', 'nor', 'nos', 'not', 'numb', 'occas', 'occass', 'occur', 'of', 'off', 'oft', 'on', 'ont', 'or', 'oth', 'out', 'ov', 'pain', 'park', 'part', 'past', 'pay', 'pee', 'peopl', 'period', 'pick', 'pink', 'piss', 'plac', 'play', 'pool', 'poop', 'prep', 'pretty', 'pric', 'project', 'prop', 'puberty', 'puk', 'purg', 'quit', 'random', 'rash', 'raz', 'real', 'rec', 'red', 'refus', 'rememb', 'rend', 'rep', 'requir', 'resta', 'retriev', 'room', 'runny', 'sad', 'sandwich', 'saw', 'say', 'school', 'seem', 'sens', 'sensationâ€i', 'sery', 'sev', 'sex', 'should', 'show', 'shredding', 'sid', 'singl', 'sint', 'skin', 'smal', 'sneez', 'sniff', 'so', 'socc', 'som', 'someon', 'someth', 'sometim', 'sor', 'sour', 'spat', 'speak', 'spot', 'standup', 'start', 'stay', 'stomach', 'straight', 'strange', 'street', 'sunk', 'suppos', 'sur', 'surrend', 'surround', 'swel', 'swim', 'swol', 'symptom', 'tak', 'talk', 'task', 'tast', 'temp', 'that', 'the', 'them', 'then', 'ther', 'thes', 'they', 'thi', 'thing', 'think', 'thos', 'though', 'thought', 'three', 'throat', 'through', 'througout', 'tight', 'tim', 'tir', 'to', 'today', 'toilet', 'too', 'touch', 'transmit', 'trip', 'troubl', 'turn', 'uncomfort', 'uncontrol', 'up', 'urin', 'useless', 'vagin', 'very', 'vessel', 'vis', 'visit', 'vomit', 'walk', 'was', 'washroom', 'we', 'weak', 'wear', 'week', 'weekend', 'weird', 'wel', 'went', 'wer', 'what', 'when', 'wher', 'which', 'whil', 'whit', 'why', 'with', 'wok', 'wont', 'worry', 'would', 'wrong', 'yellowgreen', 'yesterday', 'you', 'zoo']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
